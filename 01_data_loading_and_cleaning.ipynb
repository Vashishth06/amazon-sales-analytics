{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3070e77a-2136-45d6-9d36-e74b4f863ac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Amazon Sales Analysis - Data Loading and Cleaning\n",
    "\n",
    "Load raw sales data and perform initial cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d64929b-6567-4d42-8e2b-a7d77d1173d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CATALOG = \"db_ecom_project\"\n",
    "SCHEMA = \"amazon_sales_schema\"\n",
    "TABLE = \"amazon_sales\"\n",
    "FULL_TABLE_PATH = f\"{CATALOG}.{SCHEMA}.{TABLE}\"\n",
    "\n",
    "print(f\"ðŸ“Š Working with: {FULL_TABLE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dedbc626-44da-42bb-b6c6-200a7b354836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a111bcc-c54b-4867-9375-e4af751ce890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(FULL_TABLE_PATH)\n",
    "\n",
    "print(f\"Total records: {df.count():,}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(\"\\nSchema:\")\n",
    "df.printSchema()\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1928bd0b-27ce-43a8-a743-e5be68144a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data Quality Check\n",
    "Check for missing values and invalid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7967e842-19ba-40f1-bafa-5d01d6448e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count as spark_count, when\n",
    "\n",
    "# Null counts\n",
    "quality_check = df.select([\n",
    "    spark_count(when(col(c).isNull(), c)).alias(f\"{c}_nulls\") \n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "print(\"Null counts:\")\n",
    "display(quality_check)\n",
    "\n",
    "# Invalid checks\n",
    "invalid_price = df.filter(col(\"price\") <= 0).count()\n",
    "invalid_quantity = df.filter(col(\"quantity_sold\") <= 0).count()\n",
    "\n",
    "print(f\"\\nRows with zero/negative price: {invalid_price}\")\n",
    "print(f\"Rows with zero/negative quantity: {invalid_quantity}\")\n",
    "print(f\"Total rows before cleaning: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a2aa664-6311-452b-b57d-43e8aad07545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Data Cleaning\n",
    "Parse dates and filter invalid records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a13964c-bcff-4296-9772-de7892406b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Parse date\n",
    "df_clean = df.withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Filter valid records\n",
    "df_clean = df_clean.filter(\n",
    "    (col(\"price\") > 0) & \n",
    "    (col(\"quantity_sold\") > 0) &\n",
    "    (col(\"order_date\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"âœ… Data cleaned\")\n",
    "print(f\"Records after cleaning: {df_clean.count():,}\")\n",
    "display(df_clean.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47b63379-c24c-409f-8c7c-11217055c0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Feature Engineering\n",
    "Create date features and calculated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e236aab0-54ff-4267-ade2-a140e438bec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, quarter, dayofweek\n",
    "\n",
    "# Date features\n",
    "df_clean = df_clean \\\n",
    "    .withColumn(\"Year\", year(col(\"order_date\"))) \\\n",
    "    .withColumn(\"Month\", month(col(\"order_date\"))) \\\n",
    "    .withColumn(\"Quarter\", quarter(col(\"order_date\"))) \\\n",
    "    .withColumn(\"Day_of_Week\", dayofweek(col(\"order_date\")))\n",
    "\n",
    "# Calculated metrics\n",
    "df_clean = df_clean.withColumn(\"Revenue_Per_Unit\", col(\"total_revenue\") / col(\"quantity_sold\"))\n",
    "\n",
    "print(\"âœ… Features created\")\n",
    "display(df_clean.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4ead31d-223b-43e1-aae2-ea3a0dabd036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53432cd2-c7e5-47be-a3b1-3b29f701ca9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "cleaned_table = f\"{CATALOG}.{SCHEMA}.amazon_sales_cleaned\"\n",
    "df_clean.write.mode(\"overwrite\").saveAsTable(cleaned_table)\n",
    "\n",
    "print(f\"âœ… Saved to: {cleaned_table}\")\n",
    "print(f\"Total records: {df_clean.count():,}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_data_loading_and_cleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
